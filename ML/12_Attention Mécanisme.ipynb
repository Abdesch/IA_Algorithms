{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac794601-1995-4349-b200-4d1f22e2f1a7",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center; text-decoration:underline;\">Mécanisme d’Attention (Self-Attention)</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21eb1c84-0140-4371-8850-0920e1459dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analyse des poids d'attention ===\n",
      "Phrase originale: In this tutorial I will show you how to build embeddings and the self attention mechanism. \n",
      "\n",
      "Pour le mot 'In':\n",
      "  Attention vers 'In' : 0.1209\n",
      "  Attention vers 'embeddings' : 0.1116\n",
      "  Attention vers 'show' : 0.0992\n",
      "\n",
      "Pour le mot 'this':\n",
      "  Attention vers 'show' : 0.0890\n",
      "  Attention vers 'this' : 0.0824\n",
      "  Attention vers 'In' : 0.0799\n",
      "\n",
      "Pour le mot 'tutorial':\n",
      "  Attention vers 'tutorial' : 0.1235\n",
      "  Attention vers 'attention' : 0.1177\n",
      "  Attention vers 'will' : 0.1069\n",
      "\n",
      "Pour le mot 'I':\n",
      "  Attention vers 'attention' : 0.0885\n",
      "  Attention vers 'tutorial' : 0.0839\n",
      "  Attention vers 'and' : 0.0803\n",
      "\n",
      "Pour le mot 'will':\n",
      "  Attention vers 'tutorial' : 0.1232\n",
      "  Attention vers 'attention' : 0.1209\n",
      "  Attention vers 'will' : 0.1147\n",
      "\n",
      "Pour le mot 'show':\n",
      "  Attention vers 'show' : 0.1192\n",
      "  Attention vers 'embeddings' : 0.1134\n",
      "  Attention vers 'In' : 0.0936\n",
      "\n",
      "Pour le mot 'you':\n",
      "  Attention vers 'In' : 0.1014\n",
      "  Attention vers 'embeddings' : 0.0904\n",
      "  Attention vers 'show' : 0.0795\n",
      "\n",
      "Pour le mot 'how':\n",
      "  Attention vers 'show' : 0.0719\n",
      "  Attention vers 'this' : 0.0688\n",
      "  Attention vers 'embeddings' : 0.0687\n",
      "\n",
      "Pour le mot 'to':\n",
      "  Attention vers 'In' : 0.0915\n",
      "  Attention vers 'tutorial' : 0.0893\n",
      "  Attention vers 'embeddings' : 0.0823\n",
      "\n",
      "Pour le mot 'build':\n",
      "  Attention vers 'show' : 0.1111\n",
      "  Attention vers 'embeddings' : 0.0983\n",
      "  Attention vers 'build' : 0.0922\n",
      "\n",
      "Pour le mot 'embeddings':\n",
      "  Attention vers 'embeddings' : 0.1208\n",
      "  Attention vers 'show' : 0.1153\n",
      "  Attention vers 'In' : 0.1072\n",
      "\n",
      "Pour le mot 'and':\n",
      "  Attention vers 'attention' : 0.1031\n",
      "  Attention vers 'tutorial' : 0.0971\n",
      "  Attention vers 'will' : 0.0915\n",
      "\n",
      "Pour le mot 'the':\n",
      "  Attention vers 'show' : 0.0984\n",
      "  Attention vers 'embeddings' : 0.0883\n",
      "  Attention vers 'build' : 0.0803\n",
      "\n",
      "Pour le mot 'self':\n",
      "  Attention vers 'attention' : 0.0824\n",
      "  Attention vers 'tutorial' : 0.0797\n",
      "  Attention vers 'and' : 0.0787\n",
      "\n",
      "Pour le mot 'attention':\n",
      "  Attention vers 'attention' : 0.1259\n",
      "  Attention vers 'tutorial' : 0.1246\n",
      "  Attention vers 'will' : 0.1109\n",
      "\n",
      "Pour le mot 'mechanism.':\n",
      "  Attention vers 'In' : 0.1019\n",
      "  Attention vers 'embeddings' : 0.0892\n",
      "  Attention vers 'show' : 0.0875\n",
      "\n",
      "=== Relations d'attention principales (hors soi-même) ===\n",
      "Le mot 'In' est influencé principalement par 'embeddings' (poids: 0.1116)\n",
      "Le mot 'this' est influencé principalement par 'show' (poids: 0.0890)\n",
      "Le mot 'tutorial' est influencé principalement par 'attention' (poids: 0.1177)\n",
      "Le mot 'I' est influencé principalement par 'attention' (poids: 0.0885)\n",
      "Le mot 'will' est influencé principalement par 'tutorial' (poids: 0.1232)\n",
      "Le mot 'show' est influencé principalement par 'embeddings' (poids: 0.1134)\n",
      "Le mot 'you' est influencé principalement par 'In' (poids: 0.1014)\n",
      "Le mot 'how' est influencé principalement par 'show' (poids: 0.0719)\n",
      "Le mot 'to' est influencé principalement par 'In' (poids: 0.0915)\n",
      "Le mot 'build' est influencé principalement par 'show' (poids: 0.1111)\n",
      "Le mot 'embeddings' est influencé principalement par 'show' (poids: 0.1153)\n",
      "Le mot 'and' est influencé principalement par 'attention' (poids: 0.1031)\n",
      "Le mot 'the' est influencé principalement par 'show' (poids: 0.0984)\n",
      "Le mot 'self' est influencé principalement par 'attention' (poids: 0.0824)\n",
      "Le mot 'attention' est influencé principalement par 'tutorial' (poids: 0.1246)\n",
      "Le mot 'mechanism.' est influencé principalement par 'In' (poids: 0.1019)\n"
     ]
    }
   ],
   "source": [
    "# self_attention_demo.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============ Étape 1 : Génération des embeddings simples ============\n",
    "def build_embeddings(word):\n",
    "    np.random.seed(hash(word) % 10000)  # pour des résultats reproductibles par mot\n",
    "    return np.random.rand(4)\n",
    "\n",
    "# Exemple de phrase\n",
    "sentence = \"In this tutorial I will show you how to build embeddings and the self attention mechanism.\"\n",
    "words = sentence.split()\n",
    "embeddings = [build_embeddings(word) for word in words]\n",
    "\n",
    "# ============ Étape 2 : Fonction softmax simplifiée ============\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # pour la stabilité numérique\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# ============ Étape 3 : Mécanisme de self-attention simplifié ============\n",
    "def self_attention(embeddings):\n",
    "    Q = np.array(embeddings)\n",
    "    K = np.array(embeddings)\n",
    "    V = np.array(embeddings)\n",
    "\n",
    "    # Calcul des scores d'attention\n",
    "    scores = np.dot(Q, K.T)\n",
    "\n",
    "    # Application de softmax pour obtenir les poids d'attention\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # Calcul des valeurs pondérées\n",
    "    weighted_values = np.dot(attention_weights, V)\n",
    "    return attention_weights, weighted_values\n",
    "\n",
    "# Exécution du mécanisme\n",
    "attention_weights, weighted_values = self_attention(embeddings)\n",
    "\n",
    "# ============ Étape 4 : Analyse des poids d'attention ============\n",
    "print(\"\\n=== Analyse des poids d'attention ===\")\n",
    "print(\"Phrase originale:\", sentence, \"\\n\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"Pour le mot '{word}':\")\n",
    "    top_3 = sorted(range(len(words)), key=lambda j: attention_weights[i][j], reverse=True)[:3]\n",
    "    for j in top_3:\n",
    "        print(f\"  Attention vers '{words[j]}' : {attention_weights[i][j]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# ============ Étape 5 : Interprétation relationnelle ============\n",
    "print(\"=== Relations d'attention principales (hors soi-même) ===\")\n",
    "for i, word in enumerate(words):\n",
    "    sorted_indices = np.argsort(-attention_weights[i])\n",
    "    # Ignore le mot lui-même (attention à soi-même)\n",
    "    for idx in sorted_indices:\n",
    "        if idx != i:\n",
    "            print(f\"Le mot '{word}' est influencé principalement par '{words[idx]}' (poids: {attention_weights[i][idx]:.4f})\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cad6e4-3c7b-42c7-9a1a-5220ca39f90e",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0056b3; text-decoration:underline;\">Résultat et Interprétation</h3>\n",
    "\n",
    "Cette expérimentation met en œuvre un mécanisme de <strong>self-attention</strong> permettant à chaque mot d’une phrase de pondérer les autres mots en fonction de leur importance contextuelle.  \n",
    "Chaque mot est représenté par un vecteur numérique aléatoire, et les poids d’attention sont calculés pour chaque paire de mots via un produit scalaire entre <code>Query</code> et <code>Key</code>, normalisé ensuite par une fonction softmax.\n",
    "\n",
    "Les résultats montrent que certains mots comme <code>'attention'</code>, <code>'tutorial'</code> ou <code>'will'</code> attribuent une attention élevée à d’autres mots sémantiquement pertinents dans le contexte de la phrase.  \n",
    "D’autres termes comme <code>'show'</code> ou <code>'embeddings'</code> concentrent également l’attention, soulignant leur rôle central dans la construction du sens global.\n",
    "\n",
    "Ce comportement démontre que le mécanisme d’attention permet à un mot de “se reconstruire” dynamiquement en combinant de manière pondérée les informations issues des autres mots.  \n",
    "L’algorithme capte ainsi les <strong>relations sémantiques implicites</strong> même sans apprentissage supervisé, révélant sa puissance dans la modélisation du langage naturel.\n",
    "\n",
    "Ce test confirme l’intérêt du self-attention comme brique fondamentale des modèles de type Transformer. Il montre que la compréhension du sens global repose sur la capacité à intégrer des dépendances longues, au-delà de la proximité immédiate entre mots, ouvrant la voie à des représentations linguistiques plus expressives et contextuellement précises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97368c-345e-4ffe-95fd-c5e27da640e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
